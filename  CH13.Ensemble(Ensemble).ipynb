{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39474196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258ca7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\82107\\\\online'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재경로 확인\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f8eeff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       1       0       0       0       0       0       0       0       0   \n",
       "1   2       0       0       0       0       0       0       0       1       0   \n",
       "2   3       0       0       0       0       0       0       0       1       0   \n",
       "3   4       1       0       0       1       6       1       5       0       0   \n",
       "4   5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   ...  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  feat_91  \\\n",
       "0  ...        1        0        0        0        0        0        0   \n",
       "1  ...        0        0        0        0        0        0        0   \n",
       "2  ...        0        0        0        0        0        0        0   \n",
       "3  ...        0        1        2        0        0        0        0   \n",
       "4  ...        1        0        0        0        0        1        0   \n",
       "\n",
       "   feat_92  feat_93   target  \n",
       "0        0        0  Class_1  \n",
       "1        0        0  Class_1  \n",
       "2        0        0  Class_1  \n",
       "3        0        0  Class_1  \n",
       "4        0        0  Class_1  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"./data/otto_train.csv\") # Product Category\n",
    "data.head() # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f079cf43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid: 고유 아이디\\nfeat_1 ~ feat_93: 설명변수\\ntarget: 타겟변수 (1~9)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "id: 고유 아이디\n",
    "feat_1 ~ feat_93: 설명변수\n",
    "target: 타겟변수 (1~9)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0b1448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nCar: 61878 nVar: 95\n"
     ]
    }
   ],
   "source": [
    "nCar = data.shape[0] # 데이터 개수\n",
    "nVar = data.shape[1] # 변수 개수\n",
    "print('nCar: %d' % nCar, 'nVar: %d' % nVar )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be6c8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id'], axis = 1) # id 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8032306",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\"Class_1\": 1,\n",
    "                \"Class_2\": 2,\n",
    "                \"Class_3\": 3,\n",
    "                \"Class_4\": 4,\n",
    "                \"Class_5\": 5,\n",
    "                \"Class_6\": 6,\n",
    "                \"Class_7\": 7,\n",
    "                \"Class_8\": 8,\n",
    "                \"Class_9\": 9}\n",
    "after_mapping_target = data['target'].apply(lambda x: mapping_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0863389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49502, 93) (12376, 93) (49502,) (12376,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['target'])) # target을 제외한 모든 행\n",
    "X = data[feature_columns] # 설명변수\n",
    "y = after_mapping_target # 타겟변수\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할| \n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801baaac",
   "metadata": {},
   "source": [
    "## 1. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6570ec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:21:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 76.67 %\n",
      "Time: 9.38 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost\n",
    "import xgboost as xgb\n",
    "import time\n",
    "start = time.time() # 시작 시간 지정\n",
    "xgb_dtrain = xgb.DMatrix(data = train_x, label = train_y) # 학습 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_dtest = xgb.DMatrix(data = test_x) # 평가 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_param = {'max_depth': 10, # 트리 깊이\n",
    "         'learning_rate': 0.01, # Step Size\n",
    "         'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "         'objective': 'multi:softmax', # 목적 함수\n",
    "        'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "xgb_model = xgb.train(params = xgb_param, dtrain = xgb_dtrain) # 학습 진행\n",
    "xgb_model_predict = xgb_model.predict(xgb_dtest) # 평가 데이터 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, xgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2be4358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 6., ..., 9., 2., 7.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665547f",
   "metadata": {},
   "source": [
    "## 2. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f49a81ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3110\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.476745\n",
      "[LightGBM] [Info] Start training from score -1.341381\n",
      "[LightGBM] [Info] Start training from score -2.039019\n",
      "[LightGBM] [Info] Start training from score -3.135151\n",
      "[LightGBM] [Info] Start training from score -3.125444\n",
      "[LightGBM] [Info] Start training from score -1.481556\n",
      "[LightGBM] [Info] Start training from score -3.074772\n",
      "[LightGBM] [Info] Start training from score -1.986562\n",
      "[LightGBM] [Info] Start training from score -2.533374\n",
      "Accuracy: 76.28 %\n",
      "Time: 4.09 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'multiclass', # 목적 함수\n",
    "            'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    "lgb_model_predict = np.argmax(lgb_model.predict(test_x), axis = 1) # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, lgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae9b633b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01734061e-15, 2.25081693e-02, 3.62193933e-01, ...,\n",
       "        3.24234521e-02, 5.82126692e-02, 3.67722414e-02],\n",
       "       [1.14084116e-15, 5.36978636e-02, 1.90687128e-01, ...,\n",
       "        3.25081119e-01, 9.38028846e-02, 6.50463131e-02],\n",
       "       [5.94595781e-16, 9.66842220e-03, 5.82817482e-02, ...,\n",
       "        1.42318289e-02, 3.40230275e-02, 2.14919364e-02],\n",
       "       ...,\n",
       "       [7.09105769e-16, 4.63740004e-02, 1.08297559e-01, ...,\n",
       "        5.46934960e-02, 7.24513712e-02, 5.74635996e-01],\n",
       "       [9.88127136e-16, 1.54895684e-02, 5.45515599e-01, ...,\n",
       "        2.45870954e-02, 5.65410617e-02, 3.62344513e-02],\n",
       "       [7.59617500e-16, 1.49480877e-02, 7.44570300e-02, ...,\n",
       "        5.76695793e-01, 1.43227106e-01, 2.74567219e-02]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9f86d",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38625006",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5907034\ttotal: 861ms\tremaining: 1m 25s\n",
      "1:\tlearn: 0.6356107\ttotal: 1.53s\tremaining: 1m 15s\n",
      "2:\tlearn: 0.6411256\ttotal: 2.23s\tremaining: 1m 12s\n",
      "3:\tlearn: 0.6480344\ttotal: 2.94s\tremaining: 1m 10s\n",
      "4:\tlearn: 0.6508222\ttotal: 3.65s\tremaining: 1m 9s\n",
      "5:\tlearn: 0.6499939\ttotal: 4.33s\tremaining: 1m 7s\n",
      "6:\tlearn: 0.6507818\ttotal: 5.03s\tremaining: 1m 6s\n",
      "7:\tlearn: 0.6548422\ttotal: 5.73s\tremaining: 1m 5s\n",
      "8:\tlearn: 0.6559533\ttotal: 6.44s\tremaining: 1m 5s\n",
      "9:\tlearn: 0.6560947\ttotal: 7.16s\tremaining: 1m 4s\n",
      "10:\tlearn: 0.6568421\ttotal: 7.85s\tremaining: 1m 3s\n",
      "11:\tlearn: 0.6588219\ttotal: 8.57s\tremaining: 1m 2s\n",
      "12:\tlearn: 0.6592259\ttotal: 9.25s\tremaining: 1m 1s\n",
      "13:\tlearn: 0.6611248\ttotal: 9.96s\tremaining: 1m 1s\n",
      "14:\tlearn: 0.6625591\ttotal: 10.7s\tremaining: 1m\n",
      "15:\tlearn: 0.6631853\ttotal: 11.4s\tremaining: 59.8s\n",
      "16:\tlearn: 0.6639328\ttotal: 12.1s\tremaining: 59.2s\n",
      "17:\tlearn: 0.6668821\ttotal: 12.8s\tremaining: 58.5s\n",
      "18:\tlearn: 0.6669630\ttotal: 13.6s\tremaining: 57.8s\n",
      "19:\tlearn: 0.6675286\ttotal: 14.3s\tremaining: 57s\n",
      "20:\tlearn: 0.6673266\ttotal: 15s\tremaining: 56.3s\n",
      "21:\tlearn: 0.6677104\ttotal: 15.6s\tremaining: 55.4s\n",
      "22:\tlearn: 0.6682558\ttotal: 16.3s\tremaining: 54.4s\n",
      "23:\tlearn: 0.6683972\ttotal: 16.9s\tremaining: 53.6s\n",
      "24:\tlearn: 0.6686599\ttotal: 17.6s\tremaining: 52.8s\n",
      "25:\tlearn: 0.6681952\ttotal: 18.3s\tremaining: 52s\n",
      "26:\tlearn: 0.6684982\ttotal: 19s\tremaining: 51.3s\n",
      "27:\tlearn: 0.6692053\ttotal: 19.6s\tremaining: 50.5s\n",
      "28:\tlearn: 0.6696699\ttotal: 20.3s\tremaining: 49.8s\n",
      "29:\tlearn: 0.6699325\ttotal: 21s\tremaining: 49.1s\n",
      "30:\tlearn: 0.6705992\ttotal: 21.7s\tremaining: 48.3s\n",
      "31:\tlearn: 0.6709426\ttotal: 22.4s\tremaining: 47.6s\n",
      "32:\tlearn: 0.6708012\ttotal: 23.1s\tremaining: 46.9s\n",
      "33:\tlearn: 0.6709426\ttotal: 23.8s\tremaining: 46.2s\n",
      "34:\tlearn: 0.6707002\ttotal: 24.5s\tremaining: 45.5s\n",
      "35:\tlearn: 0.6715082\ttotal: 25.2s\tremaining: 44.8s\n",
      "36:\tlearn: 0.6705992\ttotal: 25.9s\tremaining: 44s\n",
      "37:\tlearn: 0.6725991\ttotal: 26.6s\tremaining: 43.4s\n",
      "38:\tlearn: 0.6729829\ttotal: 27.3s\tremaining: 42.7s\n",
      "39:\tlearn: 0.6725991\ttotal: 28s\tremaining: 41.9s\n",
      "40:\tlearn: 0.6734273\ttotal: 28.7s\tremaining: 41.2s\n",
      "41:\tlearn: 0.6738314\ttotal: 29.4s\tremaining: 40.6s\n",
      "42:\tlearn: 0.6741546\ttotal: 30.1s\tremaining: 39.9s\n",
      "43:\tlearn: 0.6739728\ttotal: 30.8s\tremaining: 39.2s\n",
      "44:\tlearn: 0.6741950\ttotal: 31.5s\tremaining: 38.5s\n",
      "45:\tlearn: 0.6750636\ttotal: 32.2s\tremaining: 37.8s\n",
      "46:\tlearn: 0.6758919\ttotal: 32.9s\tremaining: 37.1s\n",
      "47:\tlearn: 0.6757707\ttotal: 33.6s\tremaining: 36.4s\n",
      "48:\tlearn: 0.6762151\ttotal: 34.4s\tremaining: 35.8s\n",
      "49:\tlearn: 0.6774474\ttotal: 35.1s\tremaining: 35.1s\n",
      "50:\tlearn: 0.6777100\ttotal: 35.8s\tremaining: 34.4s\n",
      "51:\tlearn: 0.6786594\ttotal: 36.5s\tremaining: 33.7s\n",
      "52:\tlearn: 0.6789827\ttotal: 37.2s\tremaining: 33s\n",
      "53:\tlearn: 0.6804372\ttotal: 37.9s\tremaining: 32.3s\n",
      "54:\tlearn: 0.6804372\ttotal: 38.6s\tremaining: 31.6s\n",
      "55:\tlearn: 0.6809220\ttotal: 39.3s\tremaining: 30.9s\n",
      "56:\tlearn: 0.6812250\ttotal: 40s\tremaining: 30.2s\n",
      "57:\tlearn: 0.6813058\ttotal: 40.7s\tremaining: 29.5s\n",
      "58:\tlearn: 0.6811846\ttotal: 41.5s\tremaining: 28.8s\n",
      "59:\tlearn: 0.6813260\ttotal: 42.2s\tremaining: 28.1s\n",
      "60:\tlearn: 0.6816694\ttotal: 42.9s\tremaining: 27.4s\n",
      "61:\tlearn: 0.6823159\ttotal: 43.6s\tremaining: 26.7s\n",
      "62:\tlearn: 0.6832653\ttotal: 44.3s\tremaining: 26s\n",
      "63:\tlearn: 0.6840734\ttotal: 45s\tremaining: 25.3s\n",
      "64:\tlearn: 0.6840734\ttotal: 45.7s\tremaining: 24.6s\n",
      "65:\tlearn: 0.6846592\ttotal: 46.4s\tremaining: 23.9s\n",
      "66:\tlearn: 0.6843360\ttotal: 47.1s\tremaining: 23.2s\n",
      "67:\tlearn: 0.6846390\ttotal: 47.9s\tremaining: 22.5s\n",
      "68:\tlearn: 0.6854269\ttotal: 48.6s\tremaining: 21.8s\n",
      "69:\tlearn: 0.6858309\ttotal: 49.3s\tremaining: 21.1s\n",
      "70:\tlearn: 0.6858309\ttotal: 50s\tremaining: 20.4s\n",
      "71:\tlearn: 0.6865783\ttotal: 50.7s\tremaining: 19.7s\n",
      "72:\tlearn: 0.6864167\ttotal: 51.5s\tremaining: 19s\n",
      "73:\tlearn: 0.6868611\ttotal: 52.2s\tremaining: 18.3s\n",
      "74:\tlearn: 0.6869217\ttotal: 52.9s\tremaining: 17.6s\n",
      "75:\tlearn: 0.6870429\ttotal: 53.6s\tremaining: 16.9s\n",
      "76:\tlearn: 0.6875278\ttotal: 54.3s\tremaining: 16.2s\n",
      "77:\tlearn: 0.6881136\ttotal: 55s\tremaining: 15.5s\n",
      "78:\tlearn: 0.6883762\ttotal: 55.8s\tremaining: 14.8s\n",
      "79:\tlearn: 0.6888207\ttotal: 56.5s\tremaining: 14.1s\n",
      "80:\tlearn: 0.6892449\ttotal: 57.2s\tremaining: 13.4s\n",
      "81:\tlearn: 0.6898509\ttotal: 57.9s\tremaining: 12.7s\n",
      "82:\tlearn: 0.6897095\ttotal: 58.6s\tremaining: 12s\n",
      "83:\tlearn: 0.6902549\ttotal: 59.3s\tremaining: 11.3s\n",
      "84:\tlearn: 0.6909822\ttotal: 1m\tremaining: 10.6s\n",
      "85:\tlearn: 0.6910832\ttotal: 1m\tremaining: 9.89s\n",
      "86:\tlearn: 0.6914468\ttotal: 1m 1s\tremaining: 9.18s\n",
      "87:\tlearn: 0.6916084\ttotal: 1m 2s\tremaining: 8.47s\n",
      "88:\tlearn: 0.6919922\ttotal: 1m 2s\tremaining: 7.77s\n",
      "89:\tlearn: 0.6925579\ttotal: 1m 3s\tremaining: 7.06s\n",
      "90:\tlearn: 0.6928407\ttotal: 1m 4s\tremaining: 6.36s\n",
      "91:\tlearn: 0.6930427\ttotal: 1m 4s\tremaining: 5.65s\n",
      "92:\tlearn: 0.6935073\ttotal: 1m 5s\tremaining: 4.94s\n",
      "93:\tlearn: 0.6940932\ttotal: 1m 6s\tremaining: 4.23s\n",
      "94:\tlearn: 0.6944972\ttotal: 1m 7s\tremaining: 3.53s\n",
      "95:\tlearn: 0.6948810\ttotal: 1m 7s\tremaining: 2.82s\n",
      "96:\tlearn: 0.6951840\ttotal: 1m 8s\tremaining: 2.12s\n",
      "97:\tlearn: 0.6954264\ttotal: 1m 9s\tremaining: 1.41s\n",
      "98:\tlearn: 0.6955881\ttotal: 1m 9s\tremaining: 705ms\n",
      "99:\tlearn: 0.6956285\ttotal: 1m 10s\tremaining: 0us\n",
      "Accuracy: 69.64 %\n",
      "Time: 70.78 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install catboost\n",
    "import catboost as cb\n",
    "start = time.time() # 시작 시간 지정\n",
    "cb_dtrain = cb.Pool(data = train_x, label = train_y) # 학습 데이터를 Catboost 모델에 맞게 변환\n",
    "cb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'eval_metric': 'Accuracy', # 평가 척도\n",
    "            'loss_function': 'MultiClass'} # 손실 함수, 목적 함수\n",
    "cb_model = cb.train(pool = cb_dtrain, params = cb_param) # 학습 진행\n",
    "cb_model_predict = np.argmax(cb_model.predict(test_x), axis = 1) + 1 # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측, 인덱스의 순서를 맞추기 위해 +1\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, cb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c65b10a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  floors  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00     1.0   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25     2.0   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00     1.0   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00     1.0   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00     1.0   \n",
       "\n",
       "   waterfront  condition  grade  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0           0          3      7      1955             0    98178  47.5112   \n",
       "1           0          3      7      1951          1991    98125  47.7210   \n",
       "2           0          3      6      1933             0    98028  47.7379   \n",
       "3           0          5      7      1965             0    98136  47.5208   \n",
       "4           0          3      8      1987             0    98074  47.6168   \n",
       "\n",
       "      long  \n",
       "0 -122.257  \n",
       "1 -122.319  \n",
       "2 -122.233  \n",
       "3 -122.393  \n",
       "4 -122.045  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"./data/kc_house_data.csv\") \n",
    "data.head() # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ad8ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id', 'date', 'zipcode', 'lat', 'long'], axis = 1) # id, date, zipcode, lat, long  제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af2727c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15129, 8) (6484, 8) (15129,) (6484,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['price'])) # Price를 제외한 모든 행\n",
    "X = data[feature_columns]\n",
    "y = data['price']\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 42) # 학습데이터와 평가데이터의 비율을 7:3\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a803627f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 237\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537729.263666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2b8d852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210904.17249451784"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "sqrt(mean_squared_error(lgb_model.predict(test_x),test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958191e0",
   "metadata": {},
   "source": [
    "## Ensemble의 Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6101ffb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9576\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539055.197832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215482.96482625816\n",
      "9535\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539253.879040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "218844.89275771796\n",
      "9609\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000922 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 531125.751801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212462.59859842825\n",
      "9551\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000495 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537823.032520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "214706.1062764527\n",
      "9506\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 533508.568511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "212698.84602903266\n",
      "9600\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000549 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 236\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537570.077665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214296.57342745602\n",
      "9649\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 534765.277348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212666.73539342856\n",
      "9503\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535172.330293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213028.75062543826\n",
      "9583\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 230\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 536141.229691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216442.78578366552\n",
      "9565\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535324.127834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215896.94776692343\n",
      "9520\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 536497.029810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "211470.18833188174\n",
      "9519\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539977.832573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211950.35912361243\n",
      "9532\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 230\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 533907.536123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216234.84259406765\n",
      "9538\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537111.800978\n",
      "218532.08244757855\n",
      "9625\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 533063.859079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "211443.5087900497\n",
      "9527\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535303.277348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213041.15357250965\n",
      "9604\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537656.258444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "213192.5776238879\n",
      "9599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 533305.926168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217600.1438490003\n",
      "9572\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 540961.655760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212771.2169401849\n",
      "9529\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537280.923590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211516.05750983686\n",
      "9623\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000533 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 540267.109591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212855.7160651232\n",
      "9579\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 236\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539644.275035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213790.2869627441\n",
      "9513\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 534212.295459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214879.25598555457\n",
      "9555\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 230\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 530674.062926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211768.10235887178\n",
      "9581\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 534659.056646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "218358.3003269065\n",
      "9578\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 542775.273184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212168.9231395997\n",
      "9536\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 538882.932051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212098.3800546658\n",
      "9569\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 238\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 533748.186397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212552.1465781574\n",
      "9594\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 543203.695353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213696.63487815866\n",
      "9551\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535926.047194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212648.86670154284\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "bagging_predict_result = [] # 빈 리스트 생성\n",
    "for _ in range(30):\n",
    "    data_index = [data_index for data_index in range(train_x.shape[0])] # 학습 데이터의 인덱스를 리스트로 변환\n",
    "    random_data_index = np.random.choice(data_index, train_x.shape[0]) # 데이터의 1/10 크기만큼 랜덤 샘플링, // 는 소수점을 무시하기 위함\n",
    "    print(len(set(random_data_index)))\n",
    "    lgb_dtrain = lgb.Dataset(data = train_x.iloc[random_data_index,], label = train_y.iloc[random_data_index]) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "    lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "                'learning_rate': 0.01, # Step Size\n",
    "                'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "                'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "    lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    " \n",
    "    predict1 = lgb_model.predict(test_x) # 테스트 데이터 예측\n",
    "    bagging_predict_result.append(predict1) # 반복문이 실행되기 전 빈 리스트에 결과 값 저장\n",
    "    print(sqrt(mean_squared_error(lgb_model.predict(test_x),test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45e2bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging을 바탕으로 예측한 결과값에 대한 평균을 계산\n",
    "bagging_predict = [] # 빈 리스트 생성\n",
    "for lst2_index in range(test_x.shape[0]): # 테스트 데이터 개수만큼의 반복\n",
    "    temp_predict = [] # 임시 빈 리스트 생성 (반복문 내 결과값 저장)\n",
    "    for lst_index in range(len(bagging_predict_result)): # Bagging 결과 리스트 반복\n",
    "        temp_predict.append(bagging_predict_result[lst_index][lst2_index]) # 각 Bagging 결과 예측한 값 중 같은 인덱스를 리스트에 저장\n",
    "    bagging_predict.append(np.mean(temp_predict)) # 해당 인덱스의 30개의 결과값에 대한 평균을 최종 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1b16b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[507120.3771698958,\n",
       " 630996.8553803351,\n",
       " 963330.2653155171,\n",
       " 1554197.1623360785,\n",
       " 641035.3417768573,\n",
       " 368692.0127965601,\n",
       " 698333.7462845173,\n",
       " 424961.6300008573,\n",
       " 461250.16296161636,\n",
       " 493923.2943054645,\n",
       " 636635.8639788729,\n",
       " 379454.66982751165,\n",
       " 298688.6776473843,\n",
       " 357965.82325222413,\n",
       " 342804.3982718845,\n",
       " 1332307.6500807567,\n",
       " 372489.8423335101,\n",
       " 1003038.5569976007,\n",
       " 318593.1604697275,\n",
       " 529452.172056709,\n",
       " 375229.6472056616,\n",
       " 1867181.6267533053,\n",
       " 674204.5053140178,\n",
       " 543913.5329115539,\n",
       " 504741.7501986045,\n",
       " 486203.2270421538,\n",
       " 295147.7318988114,\n",
       " 248697.75365433987,\n",
       " 476375.5240916677,\n",
       " 532616.2473676906,\n",
       " 493868.0372483749,\n",
       " 474772.5866003174,\n",
       " 464586.50751434243,\n",
       " 574886.9482534261,\n",
       " 378482.74243118387,\n",
       " 1039290.0624795578,\n",
       " 931980.7410450125,\n",
       " 528691.6055874081,\n",
       " 353546.8591484937,\n",
       " 1530425.0500956352,\n",
       " 391072.74013550446,\n",
       " 274775.16398595367,\n",
       " 510482.78962484555,\n",
       " 340483.51211239654,\n",
       " 251479.42513235816,\n",
       " 243522.42024036672,\n",
       " 329233.7839421212,\n",
       " 332661.7205045388,\n",
       " 351838.4862604179,\n",
       " 561453.1659314501,\n",
       " 370811.4415201234,\n",
       " 339359.84500587947,\n",
       " 765176.6558633718,\n",
       " 334375.8283365556,\n",
       " 464611.92034948233,\n",
       " 1706898.7709225486,\n",
       " 479040.8775964796,\n",
       " 711392.1168497258,\n",
       " 334126.0688682923,\n",
       " 649823.111675847,\n",
       " 479676.52404639154,\n",
       " 376557.20128544886,\n",
       " 298053.1553371212,\n",
       " 526460.3278566356,\n",
       " 454138.479810517,\n",
       " 283631.594643079,\n",
       " 386198.80841023126,\n",
       " 1609946.853294254,\n",
       " 480706.0599752039,\n",
       " 662294.5782085664,\n",
       " 433231.11015760707,\n",
       " 298722.6412214227,\n",
       " 762432.6880097605,\n",
       " 520545.52365338843,\n",
       " 510988.43398185703,\n",
       " 1305504.4382457514,\n",
       " 805722.313184395,\n",
       " 286186.09791320225,\n",
       " 455818.5751276391,\n",
       " 939315.2663721906,\n",
       " 640765.9652553272,\n",
       " 374589.76191551995,\n",
       " 639448.7169372762,\n",
       " 361237.05264068296,\n",
       " 823469.7882686983,\n",
       " 527173.970742202,\n",
       " 518575.4563206533,\n",
       " 553425.5945285951,\n",
       " 357751.00717173854,\n",
       " 463187.7651695582,\n",
       " 350304.9212849081,\n",
       " 397341.7585863328,\n",
       " 631685.566269948,\n",
       " 1107358.9460384264,\n",
       " 431023.93054845487,\n",
       " 492587.70606008265,\n",
       " 360301.70453072,\n",
       " 307208.67231796257,\n",
       " 812981.7374965235,\n",
       " 456997.5992080001,\n",
       " 254908.54582612883,\n",
       " 919386.7686086661,\n",
       " 975460.4296617875,\n",
       " 477580.46212473355,\n",
       " 1049647.76431832,\n",
       " 297706.38515970454,\n",
       " 489716.05356079416,\n",
       " 485041.2760423272,\n",
       " 812717.6307018751,\n",
       " 2481615.9103561277,\n",
       " 551265.9050229973,\n",
       " 321210.0119604188,\n",
       " 555188.6231789814,\n",
       " 624448.1257498602,\n",
       " 547317.3269092644,\n",
       " 334327.04912122205,\n",
       " 311814.36136106716,\n",
       " 249448.82192456536,\n",
       " 319691.5098596754,\n",
       " 342804.3982718845,\n",
       " 381036.8667650143,\n",
       " 281646.231111319,\n",
       " 342176.3553912141,\n",
       " 254951.72542367224,\n",
       " 596768.4326380495,\n",
       " 663080.9623293463,\n",
       " 275287.209627191,\n",
       " 746965.3220511297,\n",
       " 452010.871683348,\n",
       " 430060.36136561056,\n",
       " 526559.5611714452,\n",
       " 466709.91302173387,\n",
       " 411082.5798093418,\n",
       " 821427.1647397237,\n",
       " 375600.4698117426,\n",
       " 459721.33011439047,\n",
       " 388205.3738286274,\n",
       " 351259.03187357937,\n",
       " 897090.2661505992,\n",
       " 619511.2689620854,\n",
       " 516557.5443103182,\n",
       " 787128.4959182222,\n",
       " 889274.7101689045,\n",
       " 403822.2516291113,\n",
       " 256413.43443172314,\n",
       " 381750.3124815821,\n",
       " 482258.2449314089,\n",
       " 244464.10518912185,\n",
       " 410611.63094530127,\n",
       " 469886.1214870623,\n",
       " 578997.6875105572,\n",
       " 671872.906912147,\n",
       " 541768.6451775247,\n",
       " 1108772.6554051288,\n",
       " 926757.0376090042,\n",
       " 882309.7027001553,\n",
       " 596563.8456440065,\n",
       " 652906.0766039082,\n",
       " 590627.3223999616,\n",
       " 489903.2131073352,\n",
       " 642652.0456699622,\n",
       " 367808.8131819768,\n",
       " 332661.7205045388,\n",
       " 355785.95106654253,\n",
       " 362475.58728140686,\n",
       " 345538.4872182324,\n",
       " 291967.1111938956,\n",
       " 311997.51024779386,\n",
       " 446990.6867548909,\n",
       " 460988.8877111025,\n",
       " 629141.5106611805,\n",
       " 395622.32155519555,\n",
       " 464574.68352165516,\n",
       " 566939.2169923901,\n",
       " 428221.8284416197,\n",
       " 420852.72200436506,\n",
       " 355085.2189158776,\n",
       " 654223.0939912155,\n",
       " 349052.6419515571,\n",
       " 252918.11196541606,\n",
       " 312499.5589934006,\n",
       " 479849.0767811749,\n",
       " 528831.414351507,\n",
       " 670366.1687422916,\n",
       " 475720.2380126953,\n",
       " 468453.9885752163,\n",
       " 271484.7798111396,\n",
       " 429024.0055215524,\n",
       " 353807.1567412139,\n",
       " 351741.9558030842,\n",
       " 372708.9443539912,\n",
       " 651099.1689929967,\n",
       " 1604721.7779006518,\n",
       " 1283593.5803611118,\n",
       " 263836.81091715774,\n",
       " 487240.1831133844,\n",
       " 494731.5793231905,\n",
       " 1629757.425801452,\n",
       " 458856.48367514537,\n",
       " 461397.5968553527,\n",
       " 322084.6541622954,\n",
       " 384160.5358663267,\n",
       " 520790.0797951754,\n",
       " 750613.2014088024,\n",
       " 791531.183995972,\n",
       " 311166.27097536117,\n",
       " 504741.7501986045,\n",
       " 311056.50607445993,\n",
       " 509125.8636598402,\n",
       " 1406664.31600833,\n",
       " 360301.70453072,\n",
       " 417425.2447421365,\n",
       " 454689.185874301,\n",
       " 360301.70453072,\n",
       " 334602.0124518486,\n",
       " 703812.91802795,\n",
       " 783897.3916440431,\n",
       " 345340.1672340796,\n",
       " 366060.4075702638,\n",
       " 361553.2721111461,\n",
       " 1718337.2985173673,\n",
       " 537328.1072414316,\n",
       " 502587.97704681684,\n",
       " 450953.5314852445,\n",
       " 520005.63916123216,\n",
       " 750105.3129025208,\n",
       " 345001.84647423937,\n",
       " 1224349.6412596966,\n",
       " 896665.8792025878,\n",
       " 460783.6016343284,\n",
       " 357323.91148557863,\n",
       " 481167.0391705059,\n",
       " 714678.8286697237,\n",
       " 298498.68438043114,\n",
       " 349645.5200584359,\n",
       " 389050.9366971162,\n",
       " 350900.342437836,\n",
       " 355991.6747707551,\n",
       " 2480601.349339262,\n",
       " 345004.9447231676,\n",
       " 441676.3339680668,\n",
       " 460445.19358398614,\n",
       " 592907.2712800745,\n",
       " 421685.62876766495,\n",
       " 468705.8828030338,\n",
       " 308484.04172036983,\n",
       " 522583.4024973727,\n",
       " 554228.4204129184,\n",
       " 721510.8958117826,\n",
       " 844973.0293209427,\n",
       " 534868.5660006943,\n",
       " 429888.0504695159,\n",
       " 717121.3981472976,\n",
       " 351529.01574108057,\n",
       " 351741.9558030842,\n",
       " 530358.8613613216,\n",
       " 501832.3315598609,\n",
       " 467220.0735438887,\n",
       " 897119.8255306549,\n",
       " 370771.0036995927,\n",
       " 3426538.8906627493,\n",
       " 634445.3392068701,\n",
       " 769775.139882521,\n",
       " 1058980.2081443348,\n",
       " 506447.97469678416,\n",
       " 685184.7200642934,\n",
       " 918983.201224771,\n",
       " 349490.97046839417,\n",
       " 721486.9798374573,\n",
       " 437386.4955897845,\n",
       " 471012.0591735408,\n",
       " 342083.1496721216,\n",
       " 287362.0128814005,\n",
       " 443597.3767811349,\n",
       " 412389.44628363376,\n",
       " 1363354.9209654585,\n",
       " 304691.3389024295,\n",
       " 343887.8394391401,\n",
       " 524313.525379731,\n",
       " 362402.4676662915,\n",
       " 313262.51605936233,\n",
       " 506875.7278779182,\n",
       " 369833.62456536136,\n",
       " 442810.6682754005,\n",
       " 486456.6509763141,\n",
       " 447058.29224769206,\n",
       " 370900.2804686155,\n",
       " 641087.9148465763,\n",
       " 353333.22004583885,\n",
       " 315244.54733920033,\n",
       " 785291.5257434902,\n",
       " 447589.923079024,\n",
       " 257621.7363794053,\n",
       " 349880.6362729774,\n",
       " 651099.1689929967,\n",
       " 650960.7228084578,\n",
       " 492061.5142246125,\n",
       " 453650.3917901652,\n",
       " 451025.01273049606,\n",
       " 568636.5246243881,\n",
       " 471255.8561551197,\n",
       " 540304.8273269095,\n",
       " 332129.79703301314,\n",
       " 567451.4566795143,\n",
       " 343696.91127944115,\n",
       " 799777.3412241923,\n",
       " 446990.6867548909,\n",
       " 387603.39436606807,\n",
       " 332782.25820685574,\n",
       " 329782.46942027844,\n",
       " 369309.50313024415,\n",
       " 291360.29956261325,\n",
       " 863957.8254965276,\n",
       " 1637977.711248815,\n",
       " 945245.7956928395,\n",
       " 446000.9937366977,\n",
       " 815082.0386516758,\n",
       " 465546.7869412938,\n",
       " 805849.1330435413,\n",
       " 344624.04877976765,\n",
       " 401172.6689202893,\n",
       " 493201.8441384543,\n",
       " 263836.81091715774,\n",
       " 299459.1213052589,\n",
       " 465201.4254289826,\n",
       " 460988.8877111025,\n",
       " 564042.7840912447,\n",
       " 298588.317925455,\n",
       " 509531.13623423874,\n",
       " 254951.72542367224,\n",
       " 669422.0012562925,\n",
       " 300359.7032895764,\n",
       " 497865.1845106972,\n",
       " 299843.6569929959,\n",
       " 396165.5080585256,\n",
       " 482676.7623003788,\n",
       " 604171.01537712,\n",
       " 470467.36856137094,\n",
       " 929795.4643090907,\n",
       " 256765.93093565854,\n",
       " 1688194.770855895,\n",
       " 473560.95677127934,\n",
       " 439768.23680018605,\n",
       " 561084.4414178195,\n",
       " 681650.6482733744,\n",
       " 620351.8745428346,\n",
       " 342871.37834888767,\n",
       " 460124.35626332485,\n",
       " 471633.86931597284,\n",
       " 731781.5216336214,\n",
       " 279401.40579391486,\n",
       " 365589.38658478676,\n",
       " 474328.6475586053,\n",
       " 615978.4573695218,\n",
       " 340685.776351195,\n",
       " 871445.7630398342,\n",
       " 555083.0536539914,\n",
       " 936669.0913770304,\n",
       " 1004381.8498970476,\n",
       " 638172.083731565,\n",
       " 370676.2481808265,\n",
       " 798573.9780272845,\n",
       " 353930.10521784576,\n",
       " 563937.3167687301,\n",
       " 684979.180516743,\n",
       " 329787.0427901327,\n",
       " 753442.614164094,\n",
       " 398855.4571887351,\n",
       " 998221.3445746751,\n",
       " 383680.59775197634,\n",
       " 500852.45207737887,\n",
       " 649832.9343718749,\n",
       " 574703.2895999533,\n",
       " 349830.6322877558,\n",
       " 545658.0702522292,\n",
       " 378478.7213125953,\n",
       " 338300.1019556485,\n",
       " 549457.9657118351,\n",
       " 388216.49722939933,\n",
       " 417342.2937148036,\n",
       " 372669.36203773716,\n",
       " 720047.3487307646,\n",
       " 647743.8976550957,\n",
       " 436124.5016710452,\n",
       " 460327.69189325,\n",
       " 459087.6266392818,\n",
       " 298822.4846333704,\n",
       " 493873.7358826222,\n",
       " 430133.9913020001,\n",
       " 465000.4895525407,\n",
       " 563698.316228089,\n",
       " 385969.0395374081,\n",
       " 378194.7766725871,\n",
       " 410447.2224322793,\n",
       " 1449600.1567247822,\n",
       " 383680.59775197634,\n",
       " 344396.64354285283,\n",
       " 1215951.757422182,\n",
       " 665309.2619742821,\n",
       " 383803.89940588566,\n",
       " 433231.11015760707,\n",
       " 477672.287521239,\n",
       " 654680.2554503682,\n",
       " 454643.7206737456,\n",
       " 401857.3845078662,\n",
       " 432482.22705170687,\n",
       " 460988.8877111025,\n",
       " 430060.36136561056,\n",
       " 482014.8067371252,\n",
       " 468676.626790586,\n",
       " 349057.00295991334,\n",
       " 473868.3004114311,\n",
       " 616664.9748149186,\n",
       " 422110.3841925005,\n",
       " 266320.535943106,\n",
       " 380817.39064964803,\n",
       " 460931.2384379924,\n",
       " 449111.87137986894,\n",
       " 1066815.6415509072,\n",
       " 452205.47631278454,\n",
       " 396940.0033527408,\n",
       " 558531.7097515663,\n",
       " 332384.15590797947,\n",
       " 681545.3557815956,\n",
       " 404052.37088297453,\n",
       " 472785.0669399693,\n",
       " 484949.9901830587,\n",
       " 474890.6770875004,\n",
       " 549400.6991875017,\n",
       " 333615.7902233766,\n",
       " 605218.3313692273,\n",
       " 517687.7138770006,\n",
       " 702448.520638824,\n",
       " 530144.521682796,\n",
       " 517556.2950792453,\n",
       " 490457.6230603889,\n",
       " 524827.5424947151,\n",
       " 381592.31246196147,\n",
       " 362015.21276269556,\n",
       " 351015.1215065898,\n",
       " 649086.831840004,\n",
       " 381189.2079642265,\n",
       " 356665.7946224231,\n",
       " 275393.5594847762,\n",
       " 376116.13013185357,\n",
       " 385062.21841350506,\n",
       " 821427.1647397237,\n",
       " 2280968.8190321275,\n",
       " 447040.56328090443,\n",
       " 1189736.4188260196,\n",
       " 501244.47632906743,\n",
       " 266001.5859176864,\n",
       " 483272.47527930076,\n",
       " 447693.50597634155,\n",
       " 431403.11450178537,\n",
       " 678432.6439313586,\n",
       " 378266.07625295036,\n",
       " 823111.1260795171,\n",
       " 373645.3497555927,\n",
       " 273491.2154236361,\n",
       " 460741.72283227957,\n",
       " 637262.0864137119,\n",
       " 975598.7097035408,\n",
       " 372681.13310909376,\n",
       " 718156.4127035255,\n",
       " 449936.48698132497,\n",
       " 357409.61628013966,\n",
       " 539507.0370780366,\n",
       " 988619.5431701607,\n",
       " 435198.61811379826,\n",
       " 470467.36856137094,\n",
       " 349057.00295991334,\n",
       " 453438.40965362854,\n",
       " 1201879.5373512495,\n",
       " 444071.9923917768,\n",
       " 496701.3361715862,\n",
       " 385742.7590363579,\n",
       " 457433.41661030013,\n",
       " 318460.9465494628,\n",
       " 450558.91763890587,\n",
       " 302526.9959862294,\n",
       " 336262.90784453123,\n",
       " 346242.7215029818,\n",
       " 383822.2705538975,\n",
       " 446344.1029184679,\n",
       " 613043.9198312794,\n",
       " 511391.8647335606,\n",
       " 335872.8731915407,\n",
       " 774357.5947868999,\n",
       " 653290.5507796563,\n",
       " 703122.4692096567,\n",
       " 350523.67654612934,\n",
       " 428221.8284416197,\n",
       " 654269.1980640753,\n",
       " 1820275.5054705753,\n",
       " 494989.94410407345,\n",
       " 691971.5222449569,\n",
       " 368513.81689936697,\n",
       " 372193.3248566572,\n",
       " 346122.42679133726,\n",
       " 704680.6490940526,\n",
       " 479676.52404639154,\n",
       " 319893.2376139491,\n",
       " 524668.702963755,\n",
       " 349138.9410102618,\n",
       " 563151.8019471612,\n",
       " 800176.3557493611,\n",
       " 1025052.4088791959,\n",
       " 332274.69773550995,\n",
       " 534895.4580375305,\n",
       " 1220330.4011940993,\n",
       " 460988.8877111025,\n",
       " 817636.9230704966,\n",
       " 629282.2195577247,\n",
       " 460340.9257104887,\n",
       " 820395.3027469964,\n",
       " 706943.7054184471,\n",
       " 250067.09111826474,\n",
       " 376557.20128544886,\n",
       " 284873.1070828454,\n",
       " 433075.36719707464,\n",
       " 536876.963272994,\n",
       " 338494.30682096846,\n",
       " 502627.8584642007,\n",
       " 528381.3153624141,\n",
       " 414766.4923936153,\n",
       " 500877.2704137461,\n",
       " 354908.9786111174,\n",
       " 821681.5623182832,\n",
       " 342314.6904170939,\n",
       " 371277.2929611268,\n",
       " 373617.242721978,\n",
       " 1175005.6275465738,\n",
       " 459878.3081452357,\n",
       " 292151.08882002125,\n",
       " 283818.69275601604,\n",
       " 468957.85828824673,\n",
       " 817882.6914919217,\n",
       " 474000.5405732196,\n",
       " 670182.7996371572,\n",
       " 542730.6104406347,\n",
       " 452267.31178792915,\n",
       " 981607.1736608843,\n",
       " 275503.74649346503,\n",
       " 949732.52440024,\n",
       " 637018.8229078212,\n",
       " 511425.2391539482,\n",
       " 1057428.019731877,\n",
       " 470785.51367330743,\n",
       " 283534.82845416287,\n",
       " 845959.5644018534,\n",
       " 338336.30691264017,\n",
       " 628003.322334091,\n",
       " 1487622.1762062297,\n",
       " 344577.68455272145,\n",
       " 498343.8693164433,\n",
       " 446676.80879775604,\n",
       " 366689.4627725944,\n",
       " 251982.32641148273,\n",
       " 873014.5292116397,\n",
       " 437731.28787845693,\n",
       " 292990.6726421853,\n",
       " 468650.2673847807,\n",
       " 354170.75260251167,\n",
       " 291813.5502277209,\n",
       " 493340.8527017711,\n",
       " 353611.7161130725,\n",
       " 416469.8738321764,\n",
       " 392643.44206289656,\n",
       " 480359.8734908271,\n",
       " 485810.37528325495,\n",
       " 540411.7774947353,\n",
       " 785486.5462879051,\n",
       " 333640.25088639744,\n",
       " 327398.686975496,\n",
       " 381189.2079642265,\n",
       " 384160.5358663267,\n",
       " 328611.93447138055,\n",
       " 461480.8643738308,\n",
       " 768406.7260426957,\n",
       " 501501.4108761241,\n",
       " 848787.1187865231,\n",
       " 848621.9654691679,\n",
       " 472500.56686578604,\n",
       " 442678.66870564,\n",
       " 479107.7671546481,\n",
       " 316392.68188066117,\n",
       " 402350.07853514346,\n",
       " 439570.5697435511,\n",
       " 321641.7160148968,\n",
       " 470851.2423085554,\n",
       " 670182.7996371572,\n",
       " 421745.4430219873,\n",
       " 407828.7433927954,\n",
       " 463212.17577310116,\n",
       " 414595.7066287987,\n",
       " 1020430.5010468197,\n",
       " 818028.1538058693,\n",
       " 738173.6393850908,\n",
       " 365826.0697591989,\n",
       " 459087.6266392818,\n",
       " 402053.7063455849,\n",
       " 563699.9133190027,\n",
       " 267140.9571639683,\n",
       " 390454.46686878265,\n",
       " 356946.94282073836,\n",
       " 316147.72125721147,\n",
       " 358484.8628554303,\n",
       " 287024.29784857464,\n",
       " 417739.2659875865,\n",
       " 593914.2047025753,\n",
       " 309390.0913571733,\n",
       " 442469.02918385115,\n",
       " 516109.85633689654,\n",
       " 487676.07220050704,\n",
       " 358146.4428087628,\n",
       " 336711.5956461261,\n",
       " 284130.71591161465,\n",
       " 536882.2000297556,\n",
       " 470488.93904483493,\n",
       " 1689793.1151437124,\n",
       " 460340.9257104887,\n",
       " 2262750.3093064753,\n",
       " 682468.4018318699,\n",
       " 1047813.6378718176,\n",
       " 613559.049898155,\n",
       " 419259.4197160128,\n",
       " 456735.6796657077,\n",
       " 974486.6331268165,\n",
       " 684990.2111150566,\n",
       " 478544.1344971778,\n",
       " 475031.73918150034,\n",
       " 742745.0043606372,\n",
       " 501315.964774567,\n",
       " 383901.4657423322,\n",
       " 402252.273807846,\n",
       " 364057.0217190179,\n",
       " 468585.34902732854,\n",
       " 339066.8925679914,\n",
       " 733829.7888989643,\n",
       " 508293.0189548538,\n",
       " 798573.9780272845,\n",
       " 706126.9370416027,\n",
       " 503323.09311095317,\n",
       " 448102.4148408032,\n",
       " 485899.7846030544,\n",
       " 491965.77337776736,\n",
       " 511044.46693394915,\n",
       " 558852.6972670226,\n",
       " 308105.68496791366,\n",
       " 1081452.4099203667,\n",
       " 311451.81887053384,\n",
       " 571866.2274471633,\n",
       " 275391.0231350132,\n",
       " 1884548.544041014,\n",
       " 843714.2144066792,\n",
       " 626370.0181912595,\n",
       " 510347.45073910936,\n",
       " 645011.1642778207,\n",
       " 554783.9266306956,\n",
       " 349228.43758619035,\n",
       " 524269.29154714843,\n",
       " 464465.23408657743,\n",
       " 996304.2782623592,\n",
       " 377989.6354317172,\n",
       " 504092.0143512047,\n",
       " 990125.4889766782,\n",
       " 535977.6615390372,\n",
       " 453739.1654209728,\n",
       " 448893.0956122264,\n",
       " 1061513.0824009685,\n",
       " 321784.6978477939,\n",
       " 631993.2584687417,\n",
       " 793478.8725877906,\n",
       " 321784.6978477939,\n",
       " 629004.3806518194,\n",
       " 594984.7519089862,\n",
       " 397213.0198934028,\n",
       " 470093.58112165314,\n",
       " 625071.9556257426,\n",
       " 437969.20815192576,\n",
       " 350523.67654612934,\n",
       " 534971.1838771828,\n",
       " 275391.0231350132,\n",
       " 347923.8676040869,\n",
       " 468423.9721708593,\n",
       " 533571.6319605949,\n",
       " 447625.30746412073,\n",
       " 820305.3209652244,\n",
       " 415248.40918274934,\n",
       " 298688.6776473843,\n",
       " 323869.94668761786,\n",
       " 468676.626790586,\n",
       " 736254.7345846231,\n",
       " 755223.7641387607,\n",
       " 370771.0036995927,\n",
       " 302129.4239225221,\n",
       " 385655.0768888643,\n",
       " 273397.011698017,\n",
       " 855576.4136223289,\n",
       " 403640.5171590632,\n",
       " 283262.62666592817,\n",
       " 338611.3218873795,\n",
       " 488534.9886684411,\n",
       " 491326.39117477386,\n",
       " 468676.626790586,\n",
       " 1127884.3453070985,\n",
       " 1008150.1517037472,\n",
       " 1194871.5437210589,\n",
       " 433173.77205578686,\n",
       " 572741.508002555,\n",
       " 366891.5257398251,\n",
       " 508929.3935042542,\n",
       " 356213.9794318437,\n",
       " 393073.3541148176,\n",
       " 346349.3178956356,\n",
       " 811886.7549540345,\n",
       " 464611.92034948233,\n",
       " 357509.4442176019,\n",
       " 394143.83746022265,\n",
       " 297009.27928556746,\n",
       " 533163.4974314517,\n",
       " 481291.54446986195,\n",
       " 426833.48035933793,\n",
       " 522683.15303012857,\n",
       " 278824.2898336831,\n",
       " 636731.6473697106,\n",
       " 522429.7073514886,\n",
       " 455818.5751276391,\n",
       " 471255.8561551197,\n",
       " 297876.507621507,\n",
       " 360443.3956867715,\n",
       " 1181159.111120464,\n",
       " 762267.8545355643,\n",
       " 442293.5047175253,\n",
       " 322320.0654692738,\n",
       " 350539.06052806193,\n",
       " 1021700.412793866,\n",
       " 552490.2654403676,\n",
       " 429720.2365050838,\n",
       " 275530.76620075724,\n",
       " 378920.4495994108,\n",
       " 296013.5781975755,\n",
       " 862768.5632290017,\n",
       " 250682.05740364143,\n",
       " 323121.98062734265,\n",
       " 295063.27380636346,\n",
       " 468705.8828030338,\n",
       " 480083.61745039164,\n",
       " 598917.6849256698,\n",
       " 575667.6000453494,\n",
       " 384695.8585412296,\n",
       " 475613.9327295519,\n",
       " 563562.4748441058,\n",
       " 481152.1238576637,\n",
       " 470765.00154681824,\n",
       " 619511.2689620854,\n",
       " 510353.3002647865,\n",
       " 774980.0514121726,\n",
       " 386327.81797066436,\n",
       " 321979.91154868697,\n",
       " 466685.76500812534,\n",
       " 582629.3937904997,\n",
       " 470329.88864764455,\n",
       " 493193.1568050067,\n",
       " 931630.1661221056,\n",
       " 524091.2210837286,\n",
       " 734414.8899828567,\n",
       " 615210.0312667084,\n",
       " 829764.9357520174,\n",
       " 275883.865077423,\n",
       " 480547.88985011895,\n",
       " 466075.66076687624,\n",
       " 815043.7895317446,\n",
       " 878859.6462018617,\n",
       " 277339.51515166136,\n",
       " 256413.43443172314,\n",
       " 492529.3926293963,\n",
       " 300694.0568829732,\n",
       " 711822.6486224994,\n",
       " 389355.31599866174,\n",
       " 479386.7787440962,\n",
       " 509337.5068755791,\n",
       " 381166.18814169994,\n",
       " 295063.27380636346,\n",
       " 295267.9867576534,\n",
       " 328827.67184898973,\n",
       " 634776.7321286534,\n",
       " 285350.9160491128,\n",
       " 369376.2722635671,\n",
       " 474718.7056481716,\n",
       " 391110.83907737,\n",
       " 524827.5424947151,\n",
       " 368765.8876869772,\n",
       " 393073.3541148176,\n",
       " 464611.92034948233,\n",
       " 358103.7312446259,\n",
       " 1216151.126396159,\n",
       " 616925.7787393986,\n",
       " 329556.27596352005,\n",
       " 454232.5298807446,\n",
       " 766791.8209424797,\n",
       " 635088.1107729613,\n",
       " 271902.62153183145,\n",
       " 468276.08040791954,\n",
       " 396165.5080585256,\n",
       " 298507.726099625,\n",
       " 877944.1693626731,\n",
       " 474521.86914170784,\n",
       " 551302.9787296491,\n",
       " 474861.32914055896,\n",
       " 672465.8363238333,\n",
       " 571110.4485485655,\n",
       " 854641.2125403148,\n",
       " 662294.5782085664,\n",
       " 787508.3849220915,\n",
       " 383610.5097204406,\n",
       " 1011815.1058969109,\n",
       " 623762.8160273188,\n",
       " 776937.0005225422,\n",
       " 460001.9714396175,\n",
       " 467776.76749217865,\n",
       " 378266.07625295036,\n",
       " 464611.92034948233,\n",
       " 376557.20128544886,\n",
       " 787128.4959182222,\n",
       " 521905.2460633716,\n",
       " 487313.40934205445,\n",
       " 277528.5067115263,\n",
       " 275391.0231350132,\n",
       " 299013.0753136351,\n",
       " 815527.6698196143,\n",
       " 649823.111675847,\n",
       " 459287.0573346293,\n",
       " 363847.41877978767,\n",
       " 471016.47136698826,\n",
       " 369309.50313024415,\n",
       " 651930.8035765626,\n",
       " 275266.5246263054,\n",
       " 646015.9600080989,\n",
       " 521178.18895350565,\n",
       " 349465.2555864375,\n",
       " 276437.8229049633,\n",
       " 480706.0599752039,\n",
       " 505378.26522322354,\n",
       " 381555.5011971988,\n",
       " 692787.9955748417,\n",
       " 336262.90784453123,\n",
       " 389355.31599866174,\n",
       " 548763.5803639038,\n",
       " 302261.22769387456,\n",
       " 571363.8200628225,\n",
       " 692049.7661519897,\n",
       " 706021.6820238772,\n",
       " 489511.52098278515,\n",
       " 421907.2603111759,\n",
       " 703410.9676487446,\n",
       " 512886.10512853396,\n",
       " 649823.111675847,\n",
       " 1527263.7817620502,\n",
       " 535910.4405460139,\n",
       " 344818.3677186941,\n",
       " 696960.3036246893,\n",
       " 837590.0865962113,\n",
       " 342817.49195131584,\n",
       " 405843.90692775726,\n",
       " 374695.6696362072,\n",
       " 292186.9664173025,\n",
       " 512764.3331255561,\n",
       " 492533.90221317305,\n",
       " 411025.61488465354,\n",
       " 489820.2648343235,\n",
       " 329905.76090988977,\n",
       " 367492.2646508322,\n",
       " 251988.46009144114,\n",
       " 351259.03187357937,\n",
       " 344396.64354285283,\n",
       " 330177.293670777,\n",
       " 343592.7208314985,\n",
       " 332543.45744454546,\n",
       " 816054.8619664289,\n",
       " 421942.1635240605,\n",
       " 437461.33162999054,\n",
       " 713599.4762138283,\n",
       " 352388.3476960281,\n",
       " 250682.05740364143,\n",
       " 367903.733006146,\n",
       " 294878.2342015759,\n",
       " 582859.9014024541,\n",
       " 300485.0819925863,\n",
       " 515778.87059123046,\n",
       " 807531.4811333306,\n",
       " 575452.0119278055,\n",
       " 561192.5316140396,\n",
       " 500839.7732554146,\n",
       " 265206.01138859027,\n",
       " 994395.2727536146,\n",
       " 641035.3417768573,\n",
       " 844973.0293209427,\n",
       " 329173.27159178804,\n",
       " 463239.37297074194,\n",
       " 807026.1229759603,\n",
       " 516188.39095319086,\n",
       " 253851.92090959402,\n",
       " 1166256.8109085772,\n",
       " 312442.02555000904,\n",
       " 635677.889307345,\n",
       " 468650.2673847807,\n",
       " 775674.682935973,\n",
       " 545605.988316101,\n",
       " 449936.48698132497,\n",
       " 459087.6266392818,\n",
       " 446554.55714178534,\n",
       " 549214.0344163239,\n",
       " 1305318.5678149948,\n",
       " 477509.57773589366,\n",
       " 840448.028855916,\n",
       " 485574.6062888969,\n",
       " 380635.6629439064,\n",
       " 273865.7693205379,\n",
       " 438068.76746414474,\n",
       " 552443.6109912472,\n",
       " 488654.44078842754,\n",
       " 452558.53236942925,\n",
       " 296013.5781975755,\n",
       " 363387.5786232113,\n",
       " 349581.09281008295,\n",
       " 508362.1099255087,\n",
       " 553805.2515928023,\n",
       " 283823.28461600654,\n",
       " 380464.5536297221,\n",
       " 510343.58796036267,\n",
       " 629004.3806518194,\n",
       " 966014.3846748301,\n",
       " 663704.5566105518,\n",
       " 361218.72120762354,\n",
       " 795546.4519588674,\n",
       " 707572.8156358387,\n",
       " 847803.6233036965,\n",
       " 352152.7915827703,\n",
       " 2114913.1100498657,\n",
       " 369771.76821603737,\n",
       " 381583.64612711366,\n",
       " 506550.8740121413,\n",
       " 332394.87128715694,\n",
       " 355403.60420176847,\n",
       " 544824.7161985062,\n",
       " 658847.6290892728,\n",
       " 702826.322248825,\n",
       " 274032.2182230536,\n",
       " 349830.6322877558,\n",
       " 550825.8135261487,\n",
       " 2361708.0374193997,\n",
       " 549457.0387873501,\n",
       " 466031.2938804645,\n",
       " 915252.6222012264,\n",
       " 627727.298386257,\n",
       " 553674.7863663838,\n",
       " 816933.4087579268,\n",
       " 729209.1567145805,\n",
       " 449936.48698132497,\n",
       " 294224.0832565874,\n",
       " 439964.25110590307,\n",
       " 658556.8942670829,\n",
       " 554765.1825454404,\n",
       " 279725.71410388534,\n",
       " 298496.0248436827,\n",
       " 569792.7565629658,\n",
       " 411354.5717196507,\n",
       " 677352.2965446091,\n",
       " 493089.6741148111,\n",
       " 947539.8691759557,\n",
       " 377902.21344130306,\n",
       " 292264.70153267967,\n",
       " 393200.66193981137,\n",
       " 287033.3013291975,\n",
       " 360301.70453072,\n",
       " 342817.49195131584,\n",
       " 394434.8338054732,\n",
       " 918631.4068429071,\n",
       " 582511.4335302053,\n",
       " 363968.3012231981,\n",
       " 499271.5177798632,\n",
       " 662450.6464705844,\n",
       " 253778.2853675663,\n",
       " 530556.3939050984,\n",
       " 373207.578951881,\n",
       " 640655.29033372,\n",
       " 656170.0671263741,\n",
       " 463810.3849074621,\n",
       " 290681.136074735,\n",
       " 436533.03187493875,\n",
       " 347748.79732551693,\n",
       " 889497.6226386096,\n",
       " 359707.06652021315,\n",
       " 564932.4069750105,\n",
       " 417339.9985032838,\n",
       " 385742.7590363579,\n",
       " 350448.9840756505,\n",
       " 344023.947598458,\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8500342f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209159.42150941168"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt(mean_squared_error(bagging_predict,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e4274c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbbc41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3478bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757ba88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
